{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "RAG_Unsloth_4bit_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "194424c2-3cbc-47a1-ad10-773577048b7f"
      },
      "source": [
        "# ðŸ” RAG with Unsloth Dynamic 4-bit Quantization\n",
        "This Colab notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline with **Unslothâ€™s dynamic 4-bit quantization** for memoryâ€‘efficient LLM inference.\n\n",
        "**Pipeline Stages:**\n",
        "1. Install & Setup  \n",
        "2. Define Data Models & Utilities  \n",
        "3. Load Quantized Model (Unsloth / BitsAndBytes)  \n",
        "4. Document Processing & Embedding Indexing  \n",
        "5. Vector Retrieval  \n",
        "6. RAG Orchestration  \n",
        "7. Demo Queries ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f839447-0e79-4998-aa54-71662073dfef"
      },
      "source": [
        "### ðŸ“Œ 1) Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e11a1153-ee84-4ec9-915a-68a5dd4cde06"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q unsloth transformers accelerate sentence-transformers faiss-cpu bitsandbytes torch\n",
        "print(\"âœ… Dependencies installed (if not already).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b2cd573-b77e-4e5b-a197-e63a1ceb60cc"
      },
      "source": [
        "### ðŸ“Œ 2) Data Models & Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a1712c0-5255-4586-8da5-d712145549ff"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Any\n",
        "import numpy as np\n",
        "import gc, torch\n",
        "\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    id: str\n",
        "    content: str\n",
        "    metadata: dict\n",
        "    embedding: Optional[np.ndarray]\n",
        "    source_document: str\n",
        "    chunk_index: int\n",
        "\n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    chunk: DocumentChunk\n",
        "    relevance_score: float\n",
        "    rank: int\n",
        "\n",
        "@dataclass\n",
        "class RAGResponse:\n",
        "    query: str\n",
        "    response: str\n",
        "    retrieved_chunks: List[RetrievalResult]\n",
        "    generation_metadata: dict\n",
        "    memory_usage: dict\n",
        "\n",
        "class MemoryMonitor:\n",
        "    @staticmethod\n",
        "    def vram():\n",
        "        if not torch.cuda.is_available():\n",
        "            return {\"device\": \"cpu\", \"allocated_gb\": None, \"reserved_gb\": None}\n",
        "        dev = torch.cuda.current_device()\n",
        "        return {\n",
        "            \"device\": torch.cuda.get_device_name(dev),\n",
        "            \"allocated_gb\": round(torch.cuda.memory_allocated(dev)/(1024**3), 3),\n",
        "            \"reserved_gb\": round(torch.cuda.memory_reserved(dev)/(1024**3), 3)\n",
        "        }\n",
        "    @staticmethod\n",
        "    def cleanup():\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c9ffcbd-ec25-4ba7-b60a-fb9d38a49a3c"
      },
      "source": [
        "### ðŸ“Œ 3) Model Management â€“ Unsloth 4-bit Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "719b6f0a-dccf-4355-bd8a-f2fb8e8a07e6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "class UnslothModelManager:\n",
        "    def __init__(self, model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"):\n",
        "        self.model_name = model_name\n",
        "        self.model, self.tokenizer = None, None\n",
        "\n",
        "    def load_quantized_model(self):\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            self.model_name,\n",
        "            max_seq_length=4096,\n",
        "            dtype=\"bfloat16\",\n",
        "            load_in_4bit=True,\n",
        "        )\n",
        "        self.model, self.tokenizer = model, tokenizer\n",
        "        return model, tokenizer\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        return MemoryMonitor.vram()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ce0e3c0-b4e1-4cd0-8c94-9b7a85402a08"
      },
      "source": [
        "### ðŸ“Œ 4) Document Processing & Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd46ccf3-6a55-4007-9301-480d05a1b4bc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, uuid\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, embed_model=\"sentence-transformers/all-MiniLM-L6-v2\", chunk_size=800, overlap=200):\n",
        "        self.embedder = SentenceTransformer(embed_model)\n",
        "        self.chunk_size, self.overlap = chunk_size, overlap\n",
        "        self.chunks, self.index = [], None\n",
        "\n",
        "    def chunk_documents(self, docs):\n",
        "        out = []\n",
        "        for doc_id, text in docs:\n",
        "            tokens = text.split()\n",
        "            step = max(1, self.chunk_size - self.overlap)\n",
        "            for i in range(0, len(tokens), step):\n",
        "                chunk = \" \".join(tokens[i:i+self.chunk_size])\n",
        "                if not chunk:\n",
        "                    continue\n",
        "                out.append(DocumentChunk(str(uuid.uuid4()), chunk, {\"source\": doc_id}, None, doc_id, i//step))\n",
        "                if i + self.chunk_size >= len(tokens):\n",
        "                    break\n",
        "        self.chunks = out\n",
        "        return out\n",
        "\n",
        "    def generate_embeddings(self):\n",
        "        texts = [c.content for c in self.chunks]\n",
        "        if not texts:\n",
        "            return np.zeros((0, 384), dtype=\"float32\")\n",
        "        embeddings = self.embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "        embeddings = embeddings.astype(\"float32\")\n",
        "        # Normalize for inner product similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        for c, e in zip(self.chunks, embeddings):\n",
        "            c.embedding = e\n",
        "        return embeddings\n",
        "\n",
        "    def create_index(self):\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"No chunks to index.\")\n",
        "        d = int(self.chunks[0].embedding.shape[0])\n",
        "        index = faiss.IndexFlatIP(d)\n",
        "        embs = np.vstack([c.embedding for c in self.chunks]).astype(\"float32\")\n",
        "        index.add(embs)\n",
        "        self.index = index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ca69822-1326-42dd-985e-4e2750f05c3b"
      },
      "source": [
        "### ðŸ“Œ 5) Retrieval Component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dabea3f-b73a-4573-b24d-0419c5e1f8b1"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class VectorRetriever:\n",
        "    def __init__(self, processor: DocumentProcessor, top_k=4):\n",
        "        if processor.index is None:\n",
        "            raise ValueError(\"DocumentProcessor must have an index before creating VectorRetriever.\")\n",
        "        self.processor = processor\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def retrieve_chunks(self, query: str):\n",
        "        q_emb = self.processor.embedder.encode([query], convert_to_numpy=True)\n",
        "        q_emb = q_emb.astype(\"float32\")\n",
        "        faiss.normalize_L2(q_emb)\n",
        "        scores, idxs = self.processor.index.search(q_emb, self.top_k)\n",
        "        results = []\n",
        "        for rank, (score, idx) in enumerate(zip(scores[0], idxs[0])):\n",
        "            results.append(RetrievalResult(self.processor.chunks[idx], float(score), rank+1))\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b059539-bc05-4078-b4e5-4eacba937223"
      },
      "source": [
        "### ðŸ“Œ 6) RAG Pipeline Orchestration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ef75900-1abf-4ef3-99fb-c06fe48555d6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class RAGPipeline:\n",
        "    def __init__(self, mm: UnslothModelManager, dp: DocumentProcessor, retriever: VectorRetriever):\n",
        "        self.mm, self.dp, self.retriever = mm, dp, retriever\n",
        "\n",
        "    def format_context(self, results):\n",
        "        return \"\\n\\n\".join([f\"[Rank {r.rank} | Score {r.relevance_score:.4f} | Source {r.chunk.metadata.get('source')}]\\n{r.chunk.content}\" for r in results])\n",
        "\n",
        "    def generate_response(self, query, context, max_new_tokens=256):\n",
        "        tokenizer = self.mm.tokenizer\n",
        "        model = self.mm.model\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        prompt = (\n",
        "            \"You are a helpful assistant that answers only using the provided CONTEXT. \"\n",
        "            \"If context is insufficient, say so.\\n\\nCONTEXT:\\n\" + context + f\"\\n\\nQuestion: {query}\\nAnswer:\" \n",
        "        )\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "        text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        # Trim echoed prompt if necessary\n",
        "        if text.startswith(prompt):\n",
        "            text = text[len(prompt):].strip()\n",
        "        return text\n",
        "\n",
        "    def process_query(self, query: str):\n",
        "        results = self.retriever.retrieve_chunks(query)\n",
        "        context = self.format_context(results) if results else \"(no relevant context)\"\n",
        "        answer = self.generate_response(query, context)\n",
        "        return RAGResponse(query, answer, results, {\"model\": self.mm.model_name}, self.mm.get_memory_usage())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01d53d56-b1d1-47e5-8611-8e617b7a9e24"
      },
      "source": [
        "### ðŸ“Œ 7) Demo Queries ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a5e086b-722b-4366-b98a-0b06e2794a45"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1) Load quantized model\n",
        "mm = UnslothModelManager()\n",
        "mm.load_quantized_model()\n",
        "print(\"Model ready on:\", mm.get_memory_usage())\n",
        "\n",
        "# 2) Sample documents\n",
        "docs = [\n",
        "    (\"design.md\", \"The RAG pipeline uses Unsloth dynamic 4-bit quantization to run an LLM efficiently. \"\n",
        "                 \"It retrieves chunks from a vector store and generates grounded responses.\"),\n",
        "    (\"requirements.md\", \"Users can load quantized models, index documents, retrieve top-k chunks with scores, \"\n",
        "                        \"and generate responses grounded in context with memory monitoring.\"),\n",
        "]\n",
        "\n",
        "# 3) Build processor + index\n",
        "dp = DocumentProcessor()\n",
        "dp.chunk_documents(docs)\n",
        "dp.generate_embeddings()\n",
        "dp.create_index()\n",
        "\n",
        "# 4) Build retriever + pipeline\n",
        "retriever = VectorRetriever(dp)\n",
        "pipeline = RAGPipeline(mm, dp, retriever)\n",
        "\n",
        "# 5) Ask a question\n",
        "resp = pipeline.process_query(\"What is this pipeline about?\")\n",
        "print(\"\\n\\n=== Answer ===\\n\", resp.response)\n",
        "\n",
        "print(\"\\n=== Retrieved Chunks ===\")\n",
        "for r in resp.retrieved_chunks:\n",
        "    print(r.rank, f\"score={r.relevance_score:.4f}\", r.chunk.metadata)\n",
        "\n",
        "print(\"\\n=== VRAM ===\\n\", resp.memory_usage)\n"
      ]
    }
  ]
}